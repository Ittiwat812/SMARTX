{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier  # Import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# Function to load and label datasets\n",
    "def load_data(normal_path, xss_path):\n",
    "    with open(normal_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        normal_urls = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "    with open(xss_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        xss_urls = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "    normal_labels = [0] * len(normal_urls)  # Label 0 for normal\n",
    "    xss_labels = [1] * len(xss_urls)        # Label 1 for XSS\n",
    "\n",
    "    urls = normal_urls + xss_urls\n",
    "    labels = normal_labels + xss_labels\n",
    "\n",
    "    return pd.DataFrame({'url': urls, 'label': labels})\n",
    "\n",
    "# Load the data\n",
    "data = load_data(r'PATH/TO/Train_NonXSS.txt', r'PATH/TO/Train_XSS.txt')\n",
    "\n",
    "# Sample a small portion of the data for quicker testing (optional)\n",
    "# Uncomment the following line to sample 10% of the data\n",
    "# data = data.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Preprocess the data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(data['url']).toarray()\n",
    "\n",
    "# Remove custom URL-based features\n",
    "X_custom = X_tfidf  # Only using TF-IDF features\n",
    "y = data['label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_custom, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE to oversample the minority class (XSS)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define and tune the MLPClassifier using GridSearchCV with fewer folds and early stopping\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(100,), (128,)],  # Smaller layers for faster training\n",
    "    'alpha': [0.001],  # Keep alpha fixed to reduce complexity\n",
    "    'max_iter': [300]   # Maximum iterations\n",
    "}\n",
    "\n",
    "mlp_clf = MLPClassifier(random_state=42, early_stopping=True, verbose=True)  # Early stopping added\n",
    "\n",
    "# Perform Grid Search to tune hyperparameters with fewer folds\n",
    "grid_search = GridSearchCV(mlp_clf, param_grid, cv=2, scoring='accuracy', verbose=10)\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Best parameters found from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "# Train the best model on resampled data\n",
    "best_mlp_clf = grid_search.best_estimator_\n",
    "best_mlp_clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test data with a custom decision threshold\n",
    "y_pred_proba = best_mlp_clf.predict_proba(X_test)[:, 1]\n",
    "threshold = 0.95  # Adjust threshold\n",
    "y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the model and vectorizer for later use\n",
    "joblib.dump(best_mlp_clf, 'mlpc_xss_model_without_custom_features.pkl')\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer_without_custom_features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train V2 with K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy.sparse' has no attribute 'linalg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split, GridSearchCV, StratifiedKFold\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneural_network\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MLPClassifier\n",
      "File \u001b[1;32mc:\\Users\\Omen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\__init__.py:73\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     __check_build,\n\u001b[0;32m     71\u001b[0m     _distributor_init,\n\u001b[0;32m     72\u001b[0m )\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     76\u001b[0m _submodules \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompose\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    115\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\Omen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[1;32mc:\\Users\\Omen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\__init__.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Omen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_chunking.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[0;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Omen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Omen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _deprecate_force_all_finite\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ComplexWarning, _preserve_dia_indices_dtype\n",
      "File \u001b[1;32mc:\\Users\\Omen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspecial\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[0;32m     19\u001b[0m _NUMPY_NAMESPACE_NAMES \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray_api_compat.numpy\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21myield_namespaces\u001b[39m(include_numpy_namespaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\Omen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\fixes.py:114\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# TODO: Remove when Scipy 1.12 is the minimum supported version\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sp_base_version \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m parse_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.12.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 114\u001b[0m     _sparse_linalg_cg \u001b[38;5;241m=\u001b[39m \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241m.\u001b[39mcg\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sparse_linalg_cg\u001b[39m(A, b, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'scipy.sparse' has no attribute 'linalg'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# Function to load and label datasets\n",
    "def load_data(normal_path, xss_path):\n",
    "    \"\"\"Loads URLs from text files and assigns labels (0 for normal, 1 for XSS).\"\"\"\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not os.path.exists(normal_path) or not os.path.exists(xss_path):\n",
    "        raise FileNotFoundError(\"One or both dataset files not found. Please check the file paths.\")\n",
    "\n",
    "    # Read normal URLs\n",
    "    with open(normal_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        normal_urls = [line.strip() for line in file if line.strip()]\n",
    "    \n",
    "    # Read XSS URLs\n",
    "    with open(xss_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        xss_urls = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "    # Labels: 0 = normal, 1 = XSS\n",
    "    normal_labels = [0] * len(normal_urls)\n",
    "    xss_labels = [1] * len(xss_urls)\n",
    "\n",
    "    urls = normal_urls + xss_urls\n",
    "    labels = normal_labels + xss_labels\n",
    "\n",
    "    return pd.DataFrame({'url': urls, 'label': labels})\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "normal_file_path = r'PATH/TO/Train_NonXSS.txt'\n",
    "xss_file_path = r'PATH/TO/Train_XSS.txt'\n",
    "\n",
    "try:\n",
    "    data = load_data(normal_file_path, xss_file_path)\n",
    "    print(f\"Dataset loaded successfully. Total samples: {len(data)}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    exit()\n",
    "\n",
    "# Uncomment the following line to sample 10% of the data for faster testing\n",
    "# data = data.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Convert labels to integer type\n",
    "data['label'] = data['label'].astype(int)\n",
    "\n",
    "# TF-IDF Vectorization (converts URLs into numerical features)\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(data['url'])\n",
    "\n",
    "# Define features and target variable\n",
    "X = X_tfidf\n",
    "y = data['label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Apply SMOTE to balance dataset (handles class imbalance by oversampling minority class)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(100,), (128,)],  # Smaller layers for efficiency\n",
    "    'alpha': [0.001],  # Regularization strength\n",
    "    'max_iter': [300]  # Maximum number of iterations\n",
    "}\n",
    "\n",
    "# Initialize MLPClassifier with early stopping enabled\n",
    "mlp_clf = MLPClassifier(random_state=42, early_stopping=True, verbose=True)\n",
    "\n",
    "# Use Stratified K-Fold Cross-Validation (ensures balanced splits)\n",
    "k_folds = 5\n",
    "stratified_kfold = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform Grid Search with K-Fold Cross-Validation\n",
    "grid_search = GridSearchCV(mlp_clf, param_grid, cv=stratified_kfold, scoring='accuracy', verbose=10, n_jobs=-1)\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Retrieve best model and hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "best_mlp_clf = grid_search.best_estimator_\n",
    "print(\"\\nBest parameters found:\", best_params)\n",
    "\n",
    "# Train the best model on resampled data\n",
    "best_mlp_clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred_proba = best_mlp_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Define custom threshold for classification\n",
    "threshold = 0.95\n",
    "y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nModel Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the trained model and TF-IDF vectorizer for later use\n",
    "joblib.dump(best_mlp_clf, 'mlpc_xss_model_KFold.pkl')\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer_KFold.pkl')\n",
    "print(\"\\nModel and vectorizer saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test without featured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V33GET-GDCBV",
    "outputId": "a80ec270-c75c-48ca-eb0c-92e401730c0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model and vectorizer...\n",
      "\n",
      "Total XSS payloads detected: 19830 / 20000 (99.15%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Function to process data using TF-IDF only (without additional features)\n",
    "def process_data(urls, vectorizer):\n",
    "    # Convert URLs to TF-IDF feature vectors\n",
    "    X_test_tfidf = vectorizer.transform(urls).toarray()\n",
    "    return X_test_tfidf\n",
    "\n",
    "# Function to test the XSS detection model\n",
    "def test_model(test_data_path, vectorizer_path='tfidf_vectorizer_without_custom_features.pkl', model_path='mlpc_xss_model_without_custom_features.pkl', threshold=0.4):\n",
    "    print(\"\\nLoading model and vectorizer...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Load the pre-trained MLPClassifier model\n",
    "        model = joblib.load(model_path)\n",
    "        \n",
    "        # Load the TF-IDF vectorizer\n",
    "        vectorizer = joblib.load(vectorizer_path)\n",
    "\n",
    "        # Read the test data (list of URLs)\n",
    "        with open(test_data_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            urls = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "        # Ensure there's data to process\n",
    "        if not urls:\n",
    "            print(\"Error: No URLs found in the test file!\")\n",
    "            return None\n",
    "\n",
    "        # Process the data using only TF-IDF features\n",
    "        X_test_tfidf = process_data(urls, vectorizer)\n",
    "        \n",
    "        # Predict the probabilities using the trained MLP model\n",
    "        y_pred_proba = model.predict_proba(X_test_tfidf)[:, 1]\n",
    "\n",
    "        # Apply threshold to classify XSS vs. normal\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "        # Compute statistics\n",
    "        total_xss_detected = np.sum(y_pred)\n",
    "        total_payloads = len(y_pred)\n",
    "        percentage_detected = (total_xss_detected / total_payloads) * 100 if total_payloads > 0 else 0\n",
    "\n",
    "        # Print results\n",
    "        print(f\"Total XSS payloads detected: {total_xss_detected} / {total_payloads} ({percentage_detected:.2f}%)\\n\")\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to the test file (all URLs in this file should be XSS attempts)\n",
    "    test_data_path = r'PATH/TO/Test_Dataset\\XSS_20000_Line.txt'\n",
    "    \n",
    "    # Run the test function and count detected XSS payloads\n",
    "    y_pred = test_model(\n",
    "        test_data_path=test_data_path,\n",
    "        vectorizer_path='tfidf_vectorizer_without_custom_features.pkl',     \n",
    "        model_path='mlpc_xss_model_without_custom_features.pkl', \n",
    "        threshold= 0.99  # Adjust based on model performance\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with prefetch with multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model and vectorizer...\n",
      "\n",
      "Prefetching URLs...\n",
      "\n",
      "Processing batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:02<00:00, 17.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total XSS payloads detected: 41658 / 42000 (99.19%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import List, Optional, Tuple\n",
    "import numpy.typing as npt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def prefetch_data(file_path: str, batch_size: int = 1000) -> List[str]:\n",
    "    \"\"\"\n",
    "    Prefetch URLs from file into memory in batches.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the file containing URLs\n",
    "        batch_size: Size of each batch to read\n",
    "        \n",
    "    Returns:\n",
    "        List of URLs\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            while True:\n",
    "                batch = [next(file).strip() for _ in range(batch_size)]\n",
    "                urls.extend([url for url in batch if url])\n",
    "                if len(batch) < batch_size:\n",
    "                    break\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    return urls\n",
    "\n",
    "def process_batch(urls: List[str], vectorizer) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Process a batch of URLs using TF-IDF vectorization.\n",
    "    \n",
    "    Args:\n",
    "        urls: List of URLs to process\n",
    "        vectorizer: Trained TF-IDF vectorizer\n",
    "        \n",
    "    Returns:\n",
    "        Processed feature vectors\n",
    "    \"\"\"\n",
    "    return vectorizer.transform(urls).toarray()\n",
    "\n",
    "def predict_batch(model, X_batch: npt.NDArray, threshold: float) -> Tuple[npt.NDArray, npt.NDArray]:\n",
    "    \"\"\"\n",
    "    Make predictions on a batch of processed data.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        X_batch: Processed feature vectors\n",
    "        threshold: Classification threshold\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (predictions, probabilities)\n",
    "    \"\"\"\n",
    "    probabilities = model.predict_proba(X_batch)[:, 1]\n",
    "    predictions = (probabilities >= threshold).astype(int)\n",
    "    return predictions, probabilities\n",
    "\n",
    "def test_model(\n",
    "    test_data_path: str,\n",
    "    vectorizer_path: str = 'tfidf_vectorizer_without_custom_features.pkl',\n",
    "    model_path: str = 'mlpc_xss_model_without_custom_features.pkl',\n",
    "    threshold: float = 0.4,\n",
    "    batch_size: int = 1000\n",
    ") -> Optional[Tuple[npt.NDArray, float]]:\n",
    "    \"\"\"\n",
    "    Test the XSS detection model with batch processing.\n",
    "    \n",
    "    Args:\n",
    "        test_data_path: Path to test data file\n",
    "        vectorizer_path: Path to saved vectorizer\n",
    "        model_path: Path to saved model\n",
    "        threshold: Classification threshold\n",
    "        batch_size: Size of batches for processing\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (predictions, detection_percentage) or None if error\n",
    "    \"\"\"\n",
    "    print(\"\\nLoading model and vectorizer...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Load model and vectorizer\n",
    "        model = joblib.load(model_path)\n",
    "        vectorizer = joblib.load(vectorizer_path)\n",
    "        \n",
    "        # Prefetch all URLs\n",
    "        print(\"Prefetching URLs...\")\n",
    "        urls = prefetch_data(test_data_path, batch_size)\n",
    "        \n",
    "        if not urls:\n",
    "            print(\"Error: No URLs found in the test file!\")\n",
    "            return None\n",
    "        \n",
    "        # Initialize arrays for storing results\n",
    "        all_predictions = []\n",
    "        all_probabilities = []\n",
    "        \n",
    "        # Process data in batches with progress bar\n",
    "        total_batches = (len(urls) + batch_size - 1) // batch_size\n",
    "        print(\"\\nProcessing batches...\")\n",
    "        \n",
    "        for i in tqdm(range(0, len(urls), batch_size)):\n",
    "            # Get current batch\n",
    "            batch_urls = urls[i:i + batch_size]\n",
    "            \n",
    "            # Process batch\n",
    "            X_batch = process_batch(batch_urls, vectorizer)\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions, probabilities = predict_batch(model, X_batch, threshold)\n",
    "            \n",
    "            # Store results\n",
    "            all_predictions.extend(predictions)\n",
    "            all_probabilities.extend(probabilities)\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        all_probabilities = np.array(all_probabilities)\n",
    "        \n",
    "        # Compute statistics\n",
    "        total_xss_detected = np.sum(all_predictions)\n",
    "        total_payloads = len(all_predictions)\n",
    "        percentage_detected = (total_xss_detected / total_payloads) * 100 if total_payloads > 0 else 0\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nTotal XSS payloads detected: {total_xss_detected} / {total_payloads} ({percentage_detected:.2f}%)\\n\")\n",
    "        \n",
    "        return all_predictions, percentage_detected\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to the test file\n",
    "    test_data_path = r'PATH/TO/XSS_Final\\Train_XSS.txt'\n",
    "    \n",
    "    # Run the test function with batch processing\n",
    "    results = test_model(\n",
    "        test_data_path=test_data_path,\n",
    "        vectorizer_path='tfidf_vectorizer_without_custom_features.pkl',\n",
    "        model_path='mlpc_xss_model_without_custom_features.pkl',\n",
    "        threshold=0.99,  # Adjust based on model performance\n",
    "        batch_size=1000  # Adjust based on available memory\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
